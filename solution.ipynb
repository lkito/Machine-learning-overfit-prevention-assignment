{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import scipy.optimize as opt\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn import svm\n",
    "\n",
    "from sklearn.feature_selection import RFECV\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import GridSearchCV, cross_val_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>target</th>\n      <th>0</th>\n      <th>1</th>\n      <th>2</th>\n      <th>3</th>\n      <th>4</th>\n      <th>5</th>\n      <th>6</th>\n      <th>7</th>\n      <th>...</th>\n      <th>290</th>\n      <th>291</th>\n      <th>292</th>\n      <th>293</th>\n      <th>294</th>\n      <th>295</th>\n      <th>296</th>\n      <th>297</th>\n      <th>298</th>\n      <th>299</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>1.0</td>\n      <td>-1.067</td>\n      <td>-1.114</td>\n      <td>-0.616</td>\n      <td>0.376</td>\n      <td>1.090</td>\n      <td>0.467</td>\n      <td>-0.422</td>\n      <td>0.460</td>\n      <td>...</td>\n      <td>0.220</td>\n      <td>-0.339</td>\n      <td>0.254</td>\n      <td>-0.179</td>\n      <td>0.352</td>\n      <td>0.125</td>\n      <td>0.347</td>\n      <td>0.436</td>\n      <td>0.958</td>\n      <td>-0.824</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n      <td>0.0</td>\n      <td>-0.831</td>\n      <td>0.271</td>\n      <td>1.716</td>\n      <td>1.096</td>\n      <td>1.731</td>\n      <td>-0.197</td>\n      <td>1.904</td>\n      <td>-0.265</td>\n      <td>...</td>\n      <td>-0.765</td>\n      <td>-0.735</td>\n      <td>-1.158</td>\n      <td>2.554</td>\n      <td>0.856</td>\n      <td>-1.506</td>\n      <td>0.462</td>\n      <td>-0.029</td>\n      <td>-1.932</td>\n      <td>-0.343</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2</td>\n      <td>0.0</td>\n      <td>0.099</td>\n      <td>1.390</td>\n      <td>-0.732</td>\n      <td>-1.065</td>\n      <td>0.005</td>\n      <td>-0.081</td>\n      <td>-1.450</td>\n      <td>0.317</td>\n      <td>...</td>\n      <td>-1.311</td>\n      <td>0.799</td>\n      <td>-1.001</td>\n      <td>1.544</td>\n      <td>0.575</td>\n      <td>-0.309</td>\n      <td>-0.339</td>\n      <td>-0.148</td>\n      <td>-0.646</td>\n      <td>0.725</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>3</td>\n      <td>1.0</td>\n      <td>-0.989</td>\n      <td>-0.916</td>\n      <td>-1.343</td>\n      <td>0.145</td>\n      <td>0.543</td>\n      <td>0.636</td>\n      <td>1.127</td>\n      <td>0.189</td>\n      <td>...</td>\n      <td>-1.370</td>\n      <td>1.093</td>\n      <td>0.596</td>\n      <td>-0.589</td>\n      <td>-0.649</td>\n      <td>-0.163</td>\n      <td>-0.958</td>\n      <td>-1.081</td>\n      <td>0.805</td>\n      <td>3.401</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>4</td>\n      <td>0.0</td>\n      <td>0.811</td>\n      <td>-1.509</td>\n      <td>0.522</td>\n      <td>-0.360</td>\n      <td>-0.220</td>\n      <td>-0.959</td>\n      <td>0.334</td>\n      <td>-0.566</td>\n      <td>...</td>\n      <td>-0.178</td>\n      <td>0.718</td>\n      <td>-1.017</td>\n      <td>1.249</td>\n      <td>-0.596</td>\n      <td>-0.445</td>\n      <td>1.751</td>\n      <td>1.442</td>\n      <td>-0.393</td>\n      <td>-0.643</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows × 302 columns</p>\n</div>",
      "text/plain": "   id  target      0      1      2      3      4      5      6      7  ...  \\\n0   0     1.0 -1.067 -1.114 -0.616  0.376  1.090  0.467 -0.422  0.460  ...   \n1   1     0.0 -0.831  0.271  1.716  1.096  1.731 -0.197  1.904 -0.265  ...   \n2   2     0.0  0.099  1.390 -0.732 -1.065  0.005 -0.081 -1.450  0.317  ...   \n3   3     1.0 -0.989 -0.916 -1.343  0.145  0.543  0.636  1.127  0.189  ...   \n4   4     0.0  0.811 -1.509  0.522 -0.360 -0.220 -0.959  0.334 -0.566  ...   \n\n     290    291    292    293    294    295    296    297    298    299  \n0  0.220 -0.339  0.254 -0.179  0.352  0.125  0.347  0.436  0.958 -0.824  \n1 -0.765 -0.735 -1.158  2.554  0.856 -1.506  0.462 -0.029 -1.932 -0.343  \n2 -1.311  0.799 -1.001  1.544  0.575 -0.309 -0.339 -0.148 -0.646  0.725  \n3 -1.370  1.093  0.596 -0.589 -0.649 -0.163 -0.958 -1.081  0.805  3.401  \n4 -0.178  0.718 -1.017  1.249 -0.596 -0.445  1.751  1.442 -0.393 -0.643  \n\n[5 rows x 302 columns]"
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Training data\n",
    "train = pd.read_csv(\"/home/kito/Desktop/sem5/მანქანური სწავლება/blu/train.csv\")\n",
    "# Test data\n",
    "test = pd.read_csv(\"/home/kito/Desktop/sem5/მანქანური სწავლება/blu/test.csv\")\n",
    "\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>target</th>\n      <th>0</th>\n      <th>1</th>\n      <th>2</th>\n      <th>3</th>\n      <th>4</th>\n      <th>5</th>\n      <th>6</th>\n      <th>7</th>\n      <th>...</th>\n      <th>290</th>\n      <th>291</th>\n      <th>292</th>\n      <th>293</th>\n      <th>294</th>\n      <th>295</th>\n      <th>296</th>\n      <th>297</th>\n      <th>298</th>\n      <th>299</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>count</th>\n      <td>250.000000</td>\n      <td>250.000000</td>\n      <td>250.000000</td>\n      <td>250.000000</td>\n      <td>250.000000</td>\n      <td>250.000000</td>\n      <td>250.000000</td>\n      <td>250.000000</td>\n      <td>250.000000</td>\n      <td>250.000000</td>\n      <td>...</td>\n      <td>250.000000</td>\n      <td>250.000000</td>\n      <td>250.000000</td>\n      <td>250.000000</td>\n      <td>250.000000</td>\n      <td>250.000000</td>\n      <td>250.000000</td>\n      <td>250.000000</td>\n      <td>250.000000</td>\n      <td>250.000000</td>\n    </tr>\n    <tr>\n      <th>mean</th>\n      <td>124.500000</td>\n      <td>0.268000</td>\n      <td>-0.098064</td>\n      <td>0.001208</td>\n      <td>0.090680</td>\n      <td>-0.122248</td>\n      <td>0.011500</td>\n      <td>-0.116624</td>\n      <td>0.006932</td>\n      <td>0.100988</td>\n      <td>...</td>\n      <td>0.013052</td>\n      <td>0.007500</td>\n      <td>0.000452</td>\n      <td>0.060276</td>\n      <td>-0.090308</td>\n      <td>-0.040728</td>\n      <td>-0.002132</td>\n      <td>-0.012540</td>\n      <td>-0.039904</td>\n      <td>0.073236</td>\n    </tr>\n    <tr>\n      <th>std</th>\n      <td>72.312977</td>\n      <td>0.443806</td>\n      <td>0.996063</td>\n      <td>0.955117</td>\n      <td>0.968065</td>\n      <td>0.933001</td>\n      <td>0.945662</td>\n      <td>1.081705</td>\n      <td>1.014091</td>\n      <td>1.028042</td>\n      <td>...</td>\n      <td>1.027845</td>\n      <td>1.048169</td>\n      <td>1.026398</td>\n      <td>1.048744</td>\n      <td>1.008657</td>\n      <td>1.051273</td>\n      <td>1.065524</td>\n      <td>0.958744</td>\n      <td>0.948251</td>\n      <td>0.924989</td>\n    </tr>\n    <tr>\n      <th>min</th>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>-3.181000</td>\n      <td>-3.041000</td>\n      <td>-2.967000</td>\n      <td>-2.898000</td>\n      <td>-2.837000</td>\n      <td>-3.831000</td>\n      <td>-2.873000</td>\n      <td>-2.489000</td>\n      <td>...</td>\n      <td>-2.824000</td>\n      <td>-2.971000</td>\n      <td>-3.592000</td>\n      <td>-3.071000</td>\n      <td>-2.621000</td>\n      <td>-3.013000</td>\n      <td>-3.275000</td>\n      <td>-2.665000</td>\n      <td>-3.006000</td>\n      <td>-2.471000</td>\n    </tr>\n    <tr>\n      <th>25%</th>\n      <td>62.250000</td>\n      <td>0.000000</td>\n      <td>-0.756250</td>\n      <td>-0.624750</td>\n      <td>-0.515750</td>\n      <td>-0.695500</td>\n      <td>-0.678000</td>\n      <td>-0.758500</td>\n      <td>-0.646250</td>\n      <td>-0.589000</td>\n      <td>...</td>\n      <td>-0.764000</td>\n      <td>-0.729250</td>\n      <td>-0.699750</td>\n      <td>-0.589000</td>\n      <td>-0.701000</td>\n      <td>-0.696750</td>\n      <td>-0.555750</td>\n      <td>-0.677500</td>\n      <td>-0.719750</td>\n      <td>-0.559000</td>\n    </tr>\n    <tr>\n      <th>50%</th>\n      <td>124.500000</td>\n      <td>0.000000</td>\n      <td>-0.064500</td>\n      <td>-0.008000</td>\n      <td>0.067500</td>\n      <td>-0.090000</td>\n      <td>0.028000</td>\n      <td>-0.073500</td>\n      <td>-0.076500</td>\n      <td>0.104500</td>\n      <td>...</td>\n      <td>0.017500</td>\n      <td>0.053000</td>\n      <td>0.029500</td>\n      <td>0.042500</td>\n      <td>-0.102000</td>\n      <td>-0.057000</td>\n      <td>0.094500</td>\n      <td>-0.025500</td>\n      <td>-0.027500</td>\n      <td>0.160500</td>\n    </tr>\n    <tr>\n      <th>75%</th>\n      <td>186.750000</td>\n      <td>1.000000</td>\n      <td>0.647750</td>\n      <td>0.493250</td>\n      <td>0.716000</td>\n      <td>0.436250</td>\n      <td>0.625250</td>\n      <td>0.554250</td>\n      <td>0.676500</td>\n      <td>0.717000</td>\n      <td>...</td>\n      <td>0.690500</td>\n      <td>0.665000</td>\n      <td>0.670750</td>\n      <td>0.713250</td>\n      <td>0.532250</td>\n      <td>0.727000</td>\n      <td>0.590000</td>\n      <td>0.671000</td>\n      <td>0.717000</td>\n      <td>0.711000</td>\n    </tr>\n    <tr>\n      <th>max</th>\n      <td>249.000000</td>\n      <td>1.000000</td>\n      <td>2.347000</td>\n      <td>3.138000</td>\n      <td>2.609000</td>\n      <td>2.590000</td>\n      <td>2.413000</td>\n      <td>2.687000</td>\n      <td>2.793000</td>\n      <td>3.766000</td>\n      <td>...</td>\n      <td>2.773000</td>\n      <td>2.701000</td>\n      <td>3.193000</td>\n      <td>4.280000</td>\n      <td>2.716000</td>\n      <td>3.074000</td>\n      <td>2.626000</td>\n      <td>2.388000</td>\n      <td>2.730000</td>\n      <td>3.401000</td>\n    </tr>\n  </tbody>\n</table>\n<p>8 rows × 302 columns</p>\n</div>",
      "text/plain": "               id      target           0           1           2           3  \\\ncount  250.000000  250.000000  250.000000  250.000000  250.000000  250.000000   \nmean   124.500000    0.268000   -0.098064    0.001208    0.090680   -0.122248   \nstd     72.312977    0.443806    0.996063    0.955117    0.968065    0.933001   \nmin      0.000000    0.000000   -3.181000   -3.041000   -2.967000   -2.898000   \n25%     62.250000    0.000000   -0.756250   -0.624750   -0.515750   -0.695500   \n50%    124.500000    0.000000   -0.064500   -0.008000    0.067500   -0.090000   \n75%    186.750000    1.000000    0.647750    0.493250    0.716000    0.436250   \nmax    249.000000    1.000000    2.347000    3.138000    2.609000    2.590000   \n\n                4           5           6           7  ...         290  \\\ncount  250.000000  250.000000  250.000000  250.000000  ...  250.000000   \nmean     0.011500   -0.116624    0.006932    0.100988  ...    0.013052   \nstd      0.945662    1.081705    1.014091    1.028042  ...    1.027845   \nmin     -2.837000   -3.831000   -2.873000   -2.489000  ...   -2.824000   \n25%     -0.678000   -0.758500   -0.646250   -0.589000  ...   -0.764000   \n50%      0.028000   -0.073500   -0.076500    0.104500  ...    0.017500   \n75%      0.625250    0.554250    0.676500    0.717000  ...    0.690500   \nmax      2.413000    2.687000    2.793000    3.766000  ...    2.773000   \n\n              291         292         293         294         295         296  \\\ncount  250.000000  250.000000  250.000000  250.000000  250.000000  250.000000   \nmean     0.007500    0.000452    0.060276   -0.090308   -0.040728   -0.002132   \nstd      1.048169    1.026398    1.048744    1.008657    1.051273    1.065524   \nmin     -2.971000   -3.592000   -3.071000   -2.621000   -3.013000   -3.275000   \n25%     -0.729250   -0.699750   -0.589000   -0.701000   -0.696750   -0.555750   \n50%      0.053000    0.029500    0.042500   -0.102000   -0.057000    0.094500   \n75%      0.665000    0.670750    0.713250    0.532250    0.727000    0.590000   \nmax      2.701000    3.193000    4.280000    2.716000    3.074000    2.626000   \n\n              297         298         299  \ncount  250.000000  250.000000  250.000000  \nmean    -0.012540   -0.039904    0.073236  \nstd      0.958744    0.948251    0.924989  \nmin     -2.665000   -3.006000   -2.471000  \n25%     -0.677500   -0.719750   -0.559000  \n50%     -0.025500   -0.027500    0.160500  \n75%      0.671000    0.717000    0.711000  \nmax      2.388000    2.730000    3.401000  \n\n[8 rows x 302 columns]"
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>0</th>\n      <th>1</th>\n      <th>2</th>\n      <th>3</th>\n      <th>4</th>\n      <th>5</th>\n      <th>6</th>\n      <th>7</th>\n      <th>8</th>\n      <th>9</th>\n      <th>...</th>\n      <th>290</th>\n      <th>291</th>\n      <th>292</th>\n      <th>293</th>\n      <th>294</th>\n      <th>295</th>\n      <th>296</th>\n      <th>297</th>\n      <th>298</th>\n      <th>299</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>-1.067</td>\n      <td>-1.114</td>\n      <td>-0.616</td>\n      <td>0.376</td>\n      <td>1.090</td>\n      <td>0.467</td>\n      <td>-0.422</td>\n      <td>0.460</td>\n      <td>-0.443</td>\n      <td>-0.338</td>\n      <td>...</td>\n      <td>0.220</td>\n      <td>-0.339</td>\n      <td>0.254</td>\n      <td>-0.179</td>\n      <td>0.352</td>\n      <td>0.125</td>\n      <td>0.347</td>\n      <td>0.436</td>\n      <td>0.958</td>\n      <td>-0.824</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>-0.831</td>\n      <td>0.271</td>\n      <td>1.716</td>\n      <td>1.096</td>\n      <td>1.731</td>\n      <td>-0.197</td>\n      <td>1.904</td>\n      <td>-0.265</td>\n      <td>0.557</td>\n      <td>1.202</td>\n      <td>...</td>\n      <td>-0.765</td>\n      <td>-0.735</td>\n      <td>-1.158</td>\n      <td>2.554</td>\n      <td>0.856</td>\n      <td>-1.506</td>\n      <td>0.462</td>\n      <td>-0.029</td>\n      <td>-1.932</td>\n      <td>-0.343</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0.099</td>\n      <td>1.390</td>\n      <td>-0.732</td>\n      <td>-1.065</td>\n      <td>0.005</td>\n      <td>-0.081</td>\n      <td>-1.450</td>\n      <td>0.317</td>\n      <td>-0.624</td>\n      <td>-0.017</td>\n      <td>...</td>\n      <td>-1.311</td>\n      <td>0.799</td>\n      <td>-1.001</td>\n      <td>1.544</td>\n      <td>0.575</td>\n      <td>-0.309</td>\n      <td>-0.339</td>\n      <td>-0.148</td>\n      <td>-0.646</td>\n      <td>0.725</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>-0.989</td>\n      <td>-0.916</td>\n      <td>-1.343</td>\n      <td>0.145</td>\n      <td>0.543</td>\n      <td>0.636</td>\n      <td>1.127</td>\n      <td>0.189</td>\n      <td>-0.118</td>\n      <td>-0.638</td>\n      <td>...</td>\n      <td>-1.370</td>\n      <td>1.093</td>\n      <td>0.596</td>\n      <td>-0.589</td>\n      <td>-0.649</td>\n      <td>-0.163</td>\n      <td>-0.958</td>\n      <td>-1.081</td>\n      <td>0.805</td>\n      <td>3.401</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0.811</td>\n      <td>-1.509</td>\n      <td>0.522</td>\n      <td>-0.360</td>\n      <td>-0.220</td>\n      <td>-0.959</td>\n      <td>0.334</td>\n      <td>-0.566</td>\n      <td>-0.656</td>\n      <td>-0.499</td>\n      <td>...</td>\n      <td>-0.178</td>\n      <td>0.718</td>\n      <td>-1.017</td>\n      <td>1.249</td>\n      <td>-0.596</td>\n      <td>-0.445</td>\n      <td>1.751</td>\n      <td>1.442</td>\n      <td>-0.393</td>\n      <td>-0.643</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>245</th>\n      <td>-0.068</td>\n      <td>-0.184</td>\n      <td>-1.153</td>\n      <td>0.610</td>\n      <td>0.414</td>\n      <td>1.557</td>\n      <td>-0.234</td>\n      <td>0.950</td>\n      <td>0.896</td>\n      <td>1.416</td>\n      <td>...</td>\n      <td>1.492</td>\n      <td>1.430</td>\n      <td>-0.333</td>\n      <td>-0.200</td>\n      <td>-1.073</td>\n      <td>0.797</td>\n      <td>1.980</td>\n      <td>1.191</td>\n      <td>1.032</td>\n      <td>-0.402</td>\n    </tr>\n    <tr>\n      <th>246</th>\n      <td>-0.234</td>\n      <td>-1.373</td>\n      <td>-2.050</td>\n      <td>-0.408</td>\n      <td>-0.255</td>\n      <td>0.784</td>\n      <td>0.986</td>\n      <td>-0.891</td>\n      <td>-0.268</td>\n      <td>-0.569</td>\n      <td>...</td>\n      <td>-0.996</td>\n      <td>0.678</td>\n      <td>1.395</td>\n      <td>0.714</td>\n      <td>0.215</td>\n      <td>-0.537</td>\n      <td>-1.267</td>\n      <td>-1.021</td>\n      <td>0.747</td>\n      <td>0.128</td>\n    </tr>\n    <tr>\n      <th>247</th>\n      <td>-2.327</td>\n      <td>-1.834</td>\n      <td>-0.762</td>\n      <td>0.660</td>\n      <td>-0.858</td>\n      <td>-2.764</td>\n      <td>-0.539</td>\n      <td>-0.065</td>\n      <td>0.549</td>\n      <td>1.474</td>\n      <td>...</td>\n      <td>-1.237</td>\n      <td>-0.620</td>\n      <td>0.670</td>\n      <td>-2.010</td>\n      <td>0.438</td>\n      <td>1.972</td>\n      <td>-0.379</td>\n      <td>0.676</td>\n      <td>-1.220</td>\n      <td>-0.855</td>\n    </tr>\n    <tr>\n      <th>248</th>\n      <td>-0.451</td>\n      <td>-0.204</td>\n      <td>-0.762</td>\n      <td>0.261</td>\n      <td>0.022</td>\n      <td>-1.487</td>\n      <td>-1.122</td>\n      <td>0.141</td>\n      <td>0.369</td>\n      <td>-0.173</td>\n      <td>...</td>\n      <td>0.729</td>\n      <td>0.411</td>\n      <td>2.366</td>\n      <td>-0.021</td>\n      <td>0.160</td>\n      <td>0.045</td>\n      <td>0.208</td>\n      <td>-2.117</td>\n      <td>-0.546</td>\n      <td>-0.093</td>\n    </tr>\n    <tr>\n      <th>249</th>\n      <td>0.725</td>\n      <td>1.064</td>\n      <td>1.333</td>\n      <td>-2.863</td>\n      <td>0.203</td>\n      <td>1.898</td>\n      <td>0.434</td>\n      <td>1.207</td>\n      <td>-0.015</td>\n      <td>1.459</td>\n      <td>...</td>\n      <td>-1.028</td>\n      <td>1.081</td>\n      <td>0.607</td>\n      <td>0.550</td>\n      <td>-2.621</td>\n      <td>-0.143</td>\n      <td>-0.544</td>\n      <td>-1.690</td>\n      <td>-0.198</td>\n      <td>0.643</td>\n    </tr>\n  </tbody>\n</table>\n<p>250 rows × 300 columns</p>\n</div>",
      "text/plain": "         0      1      2      3      4      5      6      7      8      9  \\\n0   -1.067 -1.114 -0.616  0.376  1.090  0.467 -0.422  0.460 -0.443 -0.338   \n1   -0.831  0.271  1.716  1.096  1.731 -0.197  1.904 -0.265  0.557  1.202   \n2    0.099  1.390 -0.732 -1.065  0.005 -0.081 -1.450  0.317 -0.624 -0.017   \n3   -0.989 -0.916 -1.343  0.145  0.543  0.636  1.127  0.189 -0.118 -0.638   \n4    0.811 -1.509  0.522 -0.360 -0.220 -0.959  0.334 -0.566 -0.656 -0.499   \n..     ...    ...    ...    ...    ...    ...    ...    ...    ...    ...   \n245 -0.068 -0.184 -1.153  0.610  0.414  1.557 -0.234  0.950  0.896  1.416   \n246 -0.234 -1.373 -2.050 -0.408 -0.255  0.784  0.986 -0.891 -0.268 -0.569   \n247 -2.327 -1.834 -0.762  0.660 -0.858 -2.764 -0.539 -0.065  0.549  1.474   \n248 -0.451 -0.204 -0.762  0.261  0.022 -1.487 -1.122  0.141  0.369 -0.173   \n249  0.725  1.064  1.333 -2.863  0.203  1.898  0.434  1.207 -0.015  1.459   \n\n     ...    290    291    292    293    294    295    296    297    298    299  \n0    ...  0.220 -0.339  0.254 -0.179  0.352  0.125  0.347  0.436  0.958 -0.824  \n1    ... -0.765 -0.735 -1.158  2.554  0.856 -1.506  0.462 -0.029 -1.932 -0.343  \n2    ... -1.311  0.799 -1.001  1.544  0.575 -0.309 -0.339 -0.148 -0.646  0.725  \n3    ... -1.370  1.093  0.596 -0.589 -0.649 -0.163 -0.958 -1.081  0.805  3.401  \n4    ... -0.178  0.718 -1.017  1.249 -0.596 -0.445  1.751  1.442 -0.393 -0.643  \n..   ...    ...    ...    ...    ...    ...    ...    ...    ...    ...    ...  \n245  ...  1.492  1.430 -0.333 -0.200 -1.073  0.797  1.980  1.191  1.032 -0.402  \n246  ... -0.996  0.678  1.395  0.714  0.215 -0.537 -1.267 -1.021  0.747  0.128  \n247  ... -1.237 -0.620  0.670 -2.010  0.438  1.972 -0.379  0.676 -1.220 -0.855  \n248  ...  0.729  0.411  2.366 -0.021  0.160  0.045  0.208 -2.117 -0.546 -0.093  \n249  ... -1.028  1.081  0.607  0.550 -2.621 -0.143 -0.544 -1.690 -0.198  0.643  \n\n[250 rows x 300 columns]"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Separate features and target variable\n",
    "train_x = train.drop(['id', 'target'], axis=1)\n",
    "train_y = train['target']\n",
    "test_x = test.drop(['id'], axis=1)\n",
    "train_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale features by normalization to avoid bias towards columns\n",
    "def scaleFeature(dataFrame, column):\n",
    "    stdVal = np.nanstd(dataFrame[column])\n",
    "    meanVal = np.mean(dataFrame[column])\n",
    "    dataFrame[column] = (dataFrame[column] - meanVal)/stdVal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>0</th>\n      <th>1</th>\n      <th>2</th>\n      <th>3</th>\n      <th>4</th>\n      <th>5</th>\n      <th>6</th>\n      <th>7</th>\n      <th>8</th>\n      <th>9</th>\n      <th>...</th>\n      <th>290</th>\n      <th>291</th>\n      <th>292</th>\n      <th>293</th>\n      <th>294</th>\n      <th>295</th>\n      <th>296</th>\n      <th>297</th>\n      <th>298</th>\n      <th>299</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>-0.974717</td>\n      <td>-1.169956</td>\n      <td>-0.731456</td>\n      <td>0.535098</td>\n      <td>1.142759</td>\n      <td>0.540623</td>\n      <td>-0.423820</td>\n      <td>0.349920</td>\n      <td>-0.446382</td>\n      <td>-0.342796</td>\n      <td>...</td>\n      <td>0.201746</td>\n      <td>-0.331240</td>\n      <td>0.247523</td>\n      <td>-0.228612</td>\n      <td>0.439391</td>\n      <td>0.157961</td>\n      <td>0.328319</td>\n      <td>0.468780</td>\n      <td>1.054473</td>\n      <td>-0.971943</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>-0.737309</td>\n      <td>0.283037</td>\n      <td>1.682304</td>\n      <td>1.308350</td>\n      <td>1.821951</td>\n      <td>-0.074454</td>\n      <td>1.874460</td>\n      <td>-0.356719</td>\n      <td>0.675068</td>\n      <td>1.183966</td>\n      <td>...</td>\n      <td>-0.758492</td>\n      <td>-0.709799</td>\n      <td>-1.130922</td>\n      <td>2.382589</td>\n      <td>0.940068</td>\n      <td>-1.396604</td>\n      <td>0.436464</td>\n      <td>-0.017203</td>\n      <td>-1.999355</td>\n      <td>-0.450893</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0.198240</td>\n      <td>1.456971</td>\n      <td>-0.851523</td>\n      <td>-1.012478</td>\n      <td>-0.006887</td>\n      <td>0.032999</td>\n      <td>-1.439569</td>\n      <td>0.210541</td>\n      <td>-0.649364</td>\n      <td>-0.024555</td>\n      <td>...</td>\n      <td>-1.290767</td>\n      <td>0.756641</td>\n      <td>-0.977653</td>\n      <td>1.417600</td>\n      <td>0.660921</td>\n      <td>-0.255700</td>\n      <td>-0.316787</td>\n      <td>-0.141573</td>\n      <td>-0.640454</td>\n      <td>0.706032</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>-0.896252</td>\n      <td>-0.962236</td>\n      <td>-1.483945</td>\n      <td>0.287014</td>\n      <td>0.563168</td>\n      <td>0.697171</td>\n      <td>1.106720</td>\n      <td>0.085783</td>\n      <td>-0.081911</td>\n      <td>-0.640217</td>\n      <td>...</td>\n      <td>-1.348283</td>\n      <td>1.037693</td>\n      <td>0.581395</td>\n      <td>-0.620340</td>\n      <td>-0.555008</td>\n      <td>-0.116542</td>\n      <td>-0.898886</td>\n      <td>-1.116673</td>\n      <td>0.892800</td>\n      <td>3.604844</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0.914488</td>\n      <td>-1.584348</td>\n      <td>0.446442</td>\n      <td>-0.255336</td>\n      <td>-0.245293</td>\n      <td>-0.780311</td>\n      <td>0.323170</td>\n      <td>-0.650096</td>\n      <td>-0.685251</td>\n      <td>-0.502412</td>\n      <td>...</td>\n      <td>-0.186249</td>\n      <td>0.679209</td>\n      <td>-0.993273</td>\n      <td>1.135747</td>\n      <td>-0.502358</td>\n      <td>-0.385326</td>\n      <td>1.648624</td>\n      <td>1.520175</td>\n      <td>-0.373112</td>\n      <td>-0.775872</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>245</th>\n      <td>0.030243</td>\n      <td>-0.194300</td>\n      <td>-1.287284</td>\n      <td>0.786405</td>\n      <td>0.426482</td>\n      <td>1.550313</td>\n      <td>-0.238061</td>\n      <td>0.827510</td>\n      <td>1.055240</td>\n      <td>1.396126</td>\n      <td>...</td>\n      <td>1.441769</td>\n      <td>1.359851</td>\n      <td>-0.325528</td>\n      <td>-0.248677</td>\n      <td>-0.976212</td>\n      <td>0.798469</td>\n      <td>1.863972</td>\n      <td>1.257849</td>\n      <td>1.132668</td>\n      <td>-0.514806</td>\n    </tr>\n    <tr>\n      <th>246</th>\n      <td>-0.136747</td>\n      <td>-1.441671</td>\n      <td>-2.215733</td>\n      <td>-0.306886</td>\n      <td>-0.282379</td>\n      <td>0.834267</td>\n      <td>0.967400</td>\n      <td>-0.966865</td>\n      <td>-0.250128</td>\n      <td>-0.571810</td>\n      <td>...</td>\n      <td>-0.983685</td>\n      <td>0.640970</td>\n      <td>1.361407</td>\n      <td>0.624590</td>\n      <td>0.303295</td>\n      <td>-0.473015</td>\n      <td>-1.189466</td>\n      <td>-1.053966</td>\n      <td>0.831512</td>\n      <td>0.059324</td>\n    </tr>\n    <tr>\n      <th>247</th>\n      <td>-2.242234</td>\n      <td>-1.925303</td>\n      <td>-0.882575</td>\n      <td>0.840103</td>\n      <td>-0.921306</td>\n      <td>-2.452320</td>\n      <td>-0.539426</td>\n      <td>-0.161784</td>\n      <td>0.666097</td>\n      <td>1.453628</td>\n      <td>...</td>\n      <td>-1.218627</td>\n      <td>-0.599864</td>\n      <td>0.653637</td>\n      <td>-1.978012</td>\n      <td>0.524824</td>\n      <td>1.918404</td>\n      <td>-0.354402</td>\n      <td>0.719610</td>\n      <td>-1.246993</td>\n      <td>-1.005524</td>\n    </tr>\n    <tr>\n      <th>248</th>\n      <td>-0.355042</td>\n      <td>-0.215282</td>\n      <td>-0.882575</td>\n      <td>0.411593</td>\n      <td>0.011126</td>\n      <td>-1.269408</td>\n      <td>-1.115478</td>\n      <td>0.038999</td>\n      <td>0.464235</td>\n      <td>-0.179214</td>\n      <td>...</td>\n      <td>0.697950</td>\n      <td>0.385729</td>\n      <td>2.309332</td>\n      <td>-0.077654</td>\n      <td>0.248658</td>\n      <td>0.081710</td>\n      <td>0.197606</td>\n      <td>-2.199422</td>\n      <td>-0.534786</td>\n      <td>-0.180077</td>\n    </tr>\n    <tr>\n      <th>249</th>\n      <td>0.827975</td>\n      <td>1.114967</td>\n      <td>1.285876</td>\n      <td>-2.943458</td>\n      <td>0.202910</td>\n      <td>1.866188</td>\n      <td>0.421978</td>\n      <td>1.078002</td>\n      <td>0.033599</td>\n      <td>1.438757</td>\n      <td>...</td>\n      <td>-1.014881</td>\n      <td>1.026222</td>\n      <td>0.592134</td>\n      <td>0.467899</td>\n      <td>-2.514005</td>\n      <td>-0.097479</td>\n      <td>-0.509566</td>\n      <td>-1.753154</td>\n      <td>-0.167058</td>\n      <td>0.617204</td>\n    </tr>\n  </tbody>\n</table>\n<p>250 rows × 300 columns</p>\n</div>",
      "text/plain": "            0         1         2         3         4         5         6  \\\n0   -0.974717 -1.169956 -0.731456  0.535098  1.142759  0.540623 -0.423820   \n1   -0.737309  0.283037  1.682304  1.308350  1.821951 -0.074454  1.874460   \n2    0.198240  1.456971 -0.851523 -1.012478 -0.006887  0.032999 -1.439569   \n3   -0.896252 -0.962236 -1.483945  0.287014  0.563168  0.697171  1.106720   \n4    0.914488 -1.584348  0.446442 -0.255336 -0.245293 -0.780311  0.323170   \n..        ...       ...       ...       ...       ...       ...       ...   \n245  0.030243 -0.194300 -1.287284  0.786405  0.426482  1.550313 -0.238061   \n246 -0.136747 -1.441671 -2.215733 -0.306886 -0.282379  0.834267  0.967400   \n247 -2.242234 -1.925303 -0.882575  0.840103 -0.921306 -2.452320 -0.539426   \n248 -0.355042 -0.215282 -0.882575  0.411593  0.011126 -1.269408 -1.115478   \n249  0.827975  1.114967  1.285876 -2.943458  0.202910  1.866188  0.421978   \n\n            7         8         9  ...       290       291       292  \\\n0    0.349920 -0.446382 -0.342796  ...  0.201746 -0.331240  0.247523   \n1   -0.356719  0.675068  1.183966  ... -0.758492 -0.709799 -1.130922   \n2    0.210541 -0.649364 -0.024555  ... -1.290767  0.756641 -0.977653   \n3    0.085783 -0.081911 -0.640217  ... -1.348283  1.037693  0.581395   \n4   -0.650096 -0.685251 -0.502412  ... -0.186249  0.679209 -0.993273   \n..        ...       ...       ...  ...       ...       ...       ...   \n245  0.827510  1.055240  1.396126  ...  1.441769  1.359851 -0.325528   \n246 -0.966865 -0.250128 -0.571810  ... -0.983685  0.640970  1.361407   \n247 -0.161784  0.666097  1.453628  ... -1.218627 -0.599864  0.653637   \n248  0.038999  0.464235 -0.179214  ...  0.697950  0.385729  2.309332   \n249  1.078002  0.033599  1.438757  ... -1.014881  1.026222  0.592134   \n\n          293       294       295       296       297       298       299  \n0   -0.228612  0.439391  0.157961  0.328319  0.468780  1.054473 -0.971943  \n1    2.382589  0.940068 -1.396604  0.436464 -0.017203 -1.999355 -0.450893  \n2    1.417600  0.660921 -0.255700 -0.316787 -0.141573 -0.640454  0.706032  \n3   -0.620340 -0.555008 -0.116542 -0.898886 -1.116673  0.892800  3.604844  \n4    1.135747 -0.502358 -0.385326  1.648624  1.520175 -0.373112 -0.775872  \n..        ...       ...       ...       ...       ...       ...       ...  \n245 -0.248677 -0.976212  0.798469  1.863972  1.257849  1.132668 -0.514806  \n246  0.624590  0.303295 -0.473015 -1.189466 -1.053966  0.831512  0.059324  \n247 -1.978012  0.524824  1.918404 -0.354402  0.719610 -1.246993 -1.005524  \n248 -0.077654  0.248658  0.081710  0.197606 -2.199422 -0.534786 -0.180077  \n249  0.467899 -2.514005 -0.097479 -0.509566 -1.753154 -0.167058  0.617204  \n\n[250 rows x 300 columns]"
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Scale features for better accuracy\n",
    "for col in test_x.columns:\n",
    "    scaleFeature(test_x, col)\n",
    "for col in train_x.columns:\n",
    "    scaleFeature(train_x, col)\n",
    "train_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Ones</th>\n      <th>0</th>\n      <th>1</th>\n      <th>2</th>\n      <th>3</th>\n      <th>4</th>\n      <th>5</th>\n      <th>6</th>\n      <th>7</th>\n      <th>8</th>\n      <th>...</th>\n      <th>290</th>\n      <th>291</th>\n      <th>292</th>\n      <th>293</th>\n      <th>294</th>\n      <th>295</th>\n      <th>296</th>\n      <th>297</th>\n      <th>298</th>\n      <th>299</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>-0.974717</td>\n      <td>-1.169956</td>\n      <td>-0.731456</td>\n      <td>0.535098</td>\n      <td>1.142759</td>\n      <td>0.540623</td>\n      <td>-0.423820</td>\n      <td>0.349920</td>\n      <td>-0.446382</td>\n      <td>...</td>\n      <td>0.201746</td>\n      <td>-0.331240</td>\n      <td>0.247523</td>\n      <td>-0.228612</td>\n      <td>0.439391</td>\n      <td>0.157961</td>\n      <td>0.328319</td>\n      <td>0.468780</td>\n      <td>1.054473</td>\n      <td>-0.971943</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n      <td>-0.737309</td>\n      <td>0.283037</td>\n      <td>1.682304</td>\n      <td>1.308350</td>\n      <td>1.821951</td>\n      <td>-0.074454</td>\n      <td>1.874460</td>\n      <td>-0.356719</td>\n      <td>0.675068</td>\n      <td>...</td>\n      <td>-0.758492</td>\n      <td>-0.709799</td>\n      <td>-1.130922</td>\n      <td>2.382589</td>\n      <td>0.940068</td>\n      <td>-1.396604</td>\n      <td>0.436464</td>\n      <td>-0.017203</td>\n      <td>-1.999355</td>\n      <td>-0.450893</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>1</td>\n      <td>0.198240</td>\n      <td>1.456971</td>\n      <td>-0.851523</td>\n      <td>-1.012478</td>\n      <td>-0.006887</td>\n      <td>0.032999</td>\n      <td>-1.439569</td>\n      <td>0.210541</td>\n      <td>-0.649364</td>\n      <td>...</td>\n      <td>-1.290767</td>\n      <td>0.756641</td>\n      <td>-0.977653</td>\n      <td>1.417600</td>\n      <td>0.660921</td>\n      <td>-0.255700</td>\n      <td>-0.316787</td>\n      <td>-0.141573</td>\n      <td>-0.640454</td>\n      <td>0.706032</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>1</td>\n      <td>-0.896252</td>\n      <td>-0.962236</td>\n      <td>-1.483945</td>\n      <td>0.287014</td>\n      <td>0.563168</td>\n      <td>0.697171</td>\n      <td>1.106720</td>\n      <td>0.085783</td>\n      <td>-0.081911</td>\n      <td>...</td>\n      <td>-1.348283</td>\n      <td>1.037693</td>\n      <td>0.581395</td>\n      <td>-0.620340</td>\n      <td>-0.555008</td>\n      <td>-0.116542</td>\n      <td>-0.898886</td>\n      <td>-1.116673</td>\n      <td>0.892800</td>\n      <td>3.604844</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>1</td>\n      <td>0.914488</td>\n      <td>-1.584348</td>\n      <td>0.446442</td>\n      <td>-0.255336</td>\n      <td>-0.245293</td>\n      <td>-0.780311</td>\n      <td>0.323170</td>\n      <td>-0.650096</td>\n      <td>-0.685251</td>\n      <td>...</td>\n      <td>-0.186249</td>\n      <td>0.679209</td>\n      <td>-0.993273</td>\n      <td>1.135747</td>\n      <td>-0.502358</td>\n      <td>-0.385326</td>\n      <td>1.648624</td>\n      <td>1.520175</td>\n      <td>-0.373112</td>\n      <td>-0.775872</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>245</th>\n      <td>1</td>\n      <td>0.030243</td>\n      <td>-0.194300</td>\n      <td>-1.287284</td>\n      <td>0.786405</td>\n      <td>0.426482</td>\n      <td>1.550313</td>\n      <td>-0.238061</td>\n      <td>0.827510</td>\n      <td>1.055240</td>\n      <td>...</td>\n      <td>1.441769</td>\n      <td>1.359851</td>\n      <td>-0.325528</td>\n      <td>-0.248677</td>\n      <td>-0.976212</td>\n      <td>0.798469</td>\n      <td>1.863972</td>\n      <td>1.257849</td>\n      <td>1.132668</td>\n      <td>-0.514806</td>\n    </tr>\n    <tr>\n      <th>246</th>\n      <td>1</td>\n      <td>-0.136747</td>\n      <td>-1.441671</td>\n      <td>-2.215733</td>\n      <td>-0.306886</td>\n      <td>-0.282379</td>\n      <td>0.834267</td>\n      <td>0.967400</td>\n      <td>-0.966865</td>\n      <td>-0.250128</td>\n      <td>...</td>\n      <td>-0.983685</td>\n      <td>0.640970</td>\n      <td>1.361407</td>\n      <td>0.624590</td>\n      <td>0.303295</td>\n      <td>-0.473015</td>\n      <td>-1.189466</td>\n      <td>-1.053966</td>\n      <td>0.831512</td>\n      <td>0.059324</td>\n    </tr>\n    <tr>\n      <th>247</th>\n      <td>1</td>\n      <td>-2.242234</td>\n      <td>-1.925303</td>\n      <td>-0.882575</td>\n      <td>0.840103</td>\n      <td>-0.921306</td>\n      <td>-2.452320</td>\n      <td>-0.539426</td>\n      <td>-0.161784</td>\n      <td>0.666097</td>\n      <td>...</td>\n      <td>-1.218627</td>\n      <td>-0.599864</td>\n      <td>0.653637</td>\n      <td>-1.978012</td>\n      <td>0.524824</td>\n      <td>1.918404</td>\n      <td>-0.354402</td>\n      <td>0.719610</td>\n      <td>-1.246993</td>\n      <td>-1.005524</td>\n    </tr>\n    <tr>\n      <th>248</th>\n      <td>1</td>\n      <td>-0.355042</td>\n      <td>-0.215282</td>\n      <td>-0.882575</td>\n      <td>0.411593</td>\n      <td>0.011126</td>\n      <td>-1.269408</td>\n      <td>-1.115478</td>\n      <td>0.038999</td>\n      <td>0.464235</td>\n      <td>...</td>\n      <td>0.697950</td>\n      <td>0.385729</td>\n      <td>2.309332</td>\n      <td>-0.077654</td>\n      <td>0.248658</td>\n      <td>0.081710</td>\n      <td>0.197606</td>\n      <td>-2.199422</td>\n      <td>-0.534786</td>\n      <td>-0.180077</td>\n    </tr>\n    <tr>\n      <th>249</th>\n      <td>1</td>\n      <td>0.827975</td>\n      <td>1.114967</td>\n      <td>1.285876</td>\n      <td>-2.943458</td>\n      <td>0.202910</td>\n      <td>1.866188</td>\n      <td>0.421978</td>\n      <td>1.078002</td>\n      <td>0.033599</td>\n      <td>...</td>\n      <td>-1.014881</td>\n      <td>1.026222</td>\n      <td>0.592134</td>\n      <td>0.467899</td>\n      <td>-2.514005</td>\n      <td>-0.097479</td>\n      <td>-0.509566</td>\n      <td>-1.753154</td>\n      <td>-0.167058</td>\n      <td>0.617204</td>\n    </tr>\n  </tbody>\n</table>\n<p>250 rows × 301 columns</p>\n</div>",
      "text/plain": "     Ones         0         1         2         3         4         5  \\\n0       1 -0.974717 -1.169956 -0.731456  0.535098  1.142759  0.540623   \n1       1 -0.737309  0.283037  1.682304  1.308350  1.821951 -0.074454   \n2       1  0.198240  1.456971 -0.851523 -1.012478 -0.006887  0.032999   \n3       1 -0.896252 -0.962236 -1.483945  0.287014  0.563168  0.697171   \n4       1  0.914488 -1.584348  0.446442 -0.255336 -0.245293 -0.780311   \n..    ...       ...       ...       ...       ...       ...       ...   \n245     1  0.030243 -0.194300 -1.287284  0.786405  0.426482  1.550313   \n246     1 -0.136747 -1.441671 -2.215733 -0.306886 -0.282379  0.834267   \n247     1 -2.242234 -1.925303 -0.882575  0.840103 -0.921306 -2.452320   \n248     1 -0.355042 -0.215282 -0.882575  0.411593  0.011126 -1.269408   \n249     1  0.827975  1.114967  1.285876 -2.943458  0.202910  1.866188   \n\n            6         7         8  ...       290       291       292  \\\n0   -0.423820  0.349920 -0.446382  ...  0.201746 -0.331240  0.247523   \n1    1.874460 -0.356719  0.675068  ... -0.758492 -0.709799 -1.130922   \n2   -1.439569  0.210541 -0.649364  ... -1.290767  0.756641 -0.977653   \n3    1.106720  0.085783 -0.081911  ... -1.348283  1.037693  0.581395   \n4    0.323170 -0.650096 -0.685251  ... -0.186249  0.679209 -0.993273   \n..        ...       ...       ...  ...       ...       ...       ...   \n245 -0.238061  0.827510  1.055240  ...  1.441769  1.359851 -0.325528   \n246  0.967400 -0.966865 -0.250128  ... -0.983685  0.640970  1.361407   \n247 -0.539426 -0.161784  0.666097  ... -1.218627 -0.599864  0.653637   \n248 -1.115478  0.038999  0.464235  ...  0.697950  0.385729  2.309332   \n249  0.421978  1.078002  0.033599  ... -1.014881  1.026222  0.592134   \n\n          293       294       295       296       297       298       299  \n0   -0.228612  0.439391  0.157961  0.328319  0.468780  1.054473 -0.971943  \n1    2.382589  0.940068 -1.396604  0.436464 -0.017203 -1.999355 -0.450893  \n2    1.417600  0.660921 -0.255700 -0.316787 -0.141573 -0.640454  0.706032  \n3   -0.620340 -0.555008 -0.116542 -0.898886 -1.116673  0.892800  3.604844  \n4    1.135747 -0.502358 -0.385326  1.648624  1.520175 -0.373112 -0.775872  \n..        ...       ...       ...       ...       ...       ...       ...  \n245 -0.248677 -0.976212  0.798469  1.863972  1.257849  1.132668 -0.514806  \n246  0.624590  0.303295 -0.473015 -1.189466 -1.053966  0.831512  0.059324  \n247 -1.978012  0.524824  1.918404 -0.354402  0.719610 -1.246993 -1.005524  \n248 -0.077654  0.248658  0.081710  0.197606 -2.199422 -0.534786 -0.180077  \n249  0.467899 -2.514005 -0.097479 -0.509566 -1.753154 -0.167058  0.617204  \n\n[250 rows x 301 columns]"
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Insert intercept variables\n",
    "train_x.insert( 0, 'Ones', 1)\n",
    "test_x.insert( 0, 'Ones', 1)\n",
    "train_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": "/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_split.py:1978: FutureWarning: The default value of cv will change from 3 to 5 in version 0.22. Specify it explicitly to silence this warning.\n  warnings.warn(CV_WARNING, FutureWarning)\n/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_search.py:814: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal.\n  DeprecationWarning)\n"
    }
   ],
   "source": [
    "\n",
    "########### WARNING: VERY SLOW ###################\n",
    "\n",
    "# GIVES THE SAME RESULT AS L1 WITH liblinear/saga\n",
    "\n",
    "# Test saga solver with elasticnet regularization with various lambdas and l1 ratios\n",
    "parameters = {'l1_ratio' : [0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1], 'C': [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1, 5, 10], 'solver':['saga'], 'penalty':['elasticnet']}\n",
    "model=LogisticRegression(max_iter=5000, class_weight='balanced')\n",
    "clf = GridSearchCV(model, parameters)\n",
    "best_model = clf.fit(train_x,train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Best Penalty:', best_model.best_estimator_.get_params()['penalty'])\n",
    "print('Best solver:', best_model.best_estimator_.get_params()['solver'])\n",
    "print('Best C:', best_model.best_estimator_.get_params()['C'])\n",
    "print('Best l1_ratio:', best_model.best_estimator_.get_params()['l1_ratio'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": "/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_split.py:1978: FutureWarning: The default value of cv will change from 3 to 5 in version 0.22. Specify it explicitly to silence this warning.\n  warnings.warn(CV_WARNING, FutureWarning)\n/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_search.py:814: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal.\n  DeprecationWarning)\n"
    }
   ],
   "source": [
    "# Test every solver with l2 regularization with various lambdas\n",
    "parameters = {'C': [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1, 5, 10], 'solver':['newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga'], 'penalty':['l2']}\n",
    "model=LogisticRegression(max_iter=5000, class_weight='balanced')\n",
    "clf = GridSearchCV(model, parameters)\n",
    "best_model = clf.fit(train_x,train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "Best Penalty: elasticnet\nBest solver: saga\nBest C: 0.1\n"
    }
   ],
   "source": [
    "print('Best Penalty:', best_model.best_estimator_.get_params()['penalty'])\n",
    "print('Best solver:', best_model.best_estimator_.get_params()['solver'])\n",
    "print('Best C:', best_model.best_estimator_.get_params()['C'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": "/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_split.py:1978: FutureWarning: The default value of cv will change from 3 to 5 in version 0.22. Specify it explicitly to silence this warning.\n  warnings.warn(CV_WARNING, FutureWarning)\n/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  \"the coef_ did not converge\", ConvergenceWarning)\n/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  \"the coef_ did not converge\", ConvergenceWarning)\n/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  \"the coef_ did not converge\", ConvergenceWarning)\n/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_search.py:814: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal.\n  DeprecationWarning)\n"
    }
   ],
   "source": [
    "# Test liblinear and saga solvers with l1 regularization with various lambdas\n",
    "parameters = {'C': [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1, 5, 10], 'solver':['liblinear', 'saga'], 'penalty':['l1']}\n",
    "model=LogisticRegression(max_iter=5000, class_weight='balanced')\n",
    "clf = GridSearchCV(model, parameters)\n",
    "best_model = clf.fit(train_x,train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "Best Penalty: l1\nBest solver: saga\nBest C: 0.3\n"
    }
   ],
   "source": [
    "print('Best Penalty:', best_model.best_estimator_.get_params()['penalty'])\n",
    "print('Best solver:', best_model.best_estimator_.get_params()['solver'])\n",
    "print('Best C:', best_model.best_estimator_.get_params()['C'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "[False False False False  True False False False False False False False\n False  True False  True  True  True False False False False False False\n  True False False False False False  True False False  True False False\n False False False False False False False  True False False  True False\n False False  True False False False False False False False False False\n False False False  True False  True False  True False  True False False\n False  True False False False False False False  True False  True False\n False False False False False False  True  True False False False False\n False False False False False  True False False False False False False\n  True False False  True False False  True False False  True False False\n False False False False False False False  True False  True  True False\n  True  True  True False False False False False False False False False\n False  True False  True False False False False False False False False\n False False False False False False False False False  True False False\n  True False False False False False False False  True False False False\n  True False False  True False False False False False  True False False\n False False  True False False False False  True False  True False False\n False False False False False  True False  True False False False False\n False  True False False False False False False False False  True  True\n False  True  True False False False False False False  True False  True\n False False False False False False False False False  True False False\n  True False False False False False  True False False False False False\n False False False False False False False False  True False False False\n  True False False False False False False False False  True False False\n  True False False False False False False  True False False  True False]\n"
    },
    {
     "data": {
      "text/plain": "RFECV(cv=3,\n      estimator=LogisticRegression(C=0.2, class_weight='balanced', dual=False,\n                                   fit_intercept=True, intercept_scaling=1,\n                                   l1_ratio=None, max_iter=5000,\n                                   multi_class='warn', n_jobs=None,\n                                   penalty='l1', random_state=None,\n                                   solver='liblinear', tol=0.0001, verbose=0,\n                                   warm_start=False),\n      min_features_to_select=1, n_jobs=-1, scoring='roc_auc', step=0.1,\n      verbose=0)"
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model=LogisticRegression(max_iter=5000, class_weight='balanced', C=0.2, penalty='l1', solver='liblinear')\n",
    "selector = RFECV(model, step=0.1, cv=3, scoring='roc_auc', n_jobs=-1)\n",
    "selector.fit(train_x, train_y)\n",
    "print(selector.support_)\n",
    "X_new = selector.transform(train_x)\n",
    "X_test_new = selector.transform(test_x)\n",
    "selector.fit(X_new, train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "[False False False False  True  True  True  True  True  True  True  True\n  True  True  True  True  True  True  True  True  True  True  True  True\n  True  True  True  True  True  True  True  True  True  True  True  True\n  True  True  True  True  True  True  True  True  True  True  True  True\n  True  True  True  True  True  True  True  True  True  True  True  True\n  True  True  True  True  True  True False False False False False False\n False  True False False False False  True  True  True False  True False\n False False False False False False  True  True False False False False\n False False False False False  True False False False False False False\n  True False False False False False False False False  True False False\n False False False False False False False  True  True  True False False\n False  True  True False False False False False False False False False\n False False False False False False False False False False False False\n False False False False False False False False False  True False False\n False False False False False  True  True  True  True  True  True  True\n  True  True  True  True  True  True  True  True  True  True  True  True\n  True  True  True  True  True  True  True  True  True  True  True  True\n  True  True  True  True  True  True  True  True  True  True  True  True\n  True  True  True  True False False False False False False  True  True\n  True  True  True False False  True False False False  True False  True\n False False False False False False  True False False False False  True\n  True False  True  True  True  True  True  True False False False False\n False False False False False False False False  True False False False\n  True False False False False False False False False False False False\n  True False False False False False  True  True  True  True  True False]\n"
    }
   ],
   "source": [
    "model=LogisticRegression(max_iter=5000, class_weight='balanced', C=0.1, penalty='l1', solver='liblinear')\n",
    "model.fit(train_x, train_y)\n",
    "\n",
    "selector = RFE(model, step=1)\n",
    "selector.fit(train_x, train_y)\n",
    "print(selector.support_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "[[0.23699401 0.76300599]\n [0.4869367  0.5130633 ]\n [0.30969705 0.69030295]\n ...\n [0.61815862 0.38184138]\n [0.0623845  0.9376155 ]\n [0.71829035 0.28170965]]\n"
    }
   ],
   "source": [
    "# The best result seems to be with saga solver, l1 regularization and 0.1 regularization parameter\n",
    "# Use the best model to predict\n",
    "test_y=best_model.predict_proba(test_x)\n",
    "print(test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the values ready to write into file\n",
    "test_y = pd.DataFrame(test_y[:,1])\n",
    "indArr = []\n",
    "for i in range(250, 20000):\n",
    "    indArr.append(i)\n",
    "test_y.insert(0, 'id', indArr, 1)\n",
    "test_y.columns = ['id', 'target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_y.to_csv('/home/kito/Desktop/sem5/მანქანური სწავლება/blu/sample_submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}